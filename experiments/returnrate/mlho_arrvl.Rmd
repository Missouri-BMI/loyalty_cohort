---
title: "MLHO_arrvl"
output:
  html_document:
    df_print: paged
---

Changes 6/2023: The only change is to output a table of the demographic shift before and after loyalty.
To do this (assuming you already made the other changes below when you ran it before) 
 * CHANGE THE QUERY TO LOAD RACE DATA AROUND LINE 90 IF NEEDED
 * RUN THROUGH LINE 350-ish TO REBUILD THE MLHO MODEL and generate the demographics table

NOTE: To rerun this 3/2023 using LASSO, you must reinstall MLHO!
Changes 3/2023:
* Output coefficient min/max, percent of population using mlho, and prevalences of flags
* Use LASSO for MLHO analyses
* TODO: %pop on all pts , graph of changing loyalty % over score

This should be run for loyalty cohort analysis after running the loyalty score script AND the return rate script. This connects to your database and loads data via the views at the end of the return rate script.

1) CHANGE YOUR DATABASE CONNECTION INFORMATION BELOW!
2) CHANGE THE PATH TO LoyaltyCode_PSCoeff.csv BELOW!
3) SET OUTPUT DIR AND SITEID BELOW!
4) Create a results directory as a child of the working directory, so MLHO can save results.

```{r load}
# make sure the packages are installed --> 
#install.packages("devtools")
# RUN THIS ONCE 3/2023
devtools::install_github("hestiri/mlho")
if(!require(pacman)) install.packages("pacman")

# This clears the global environment! Don't run this line if you need to keep your other variables around!
rm(list=ls())

library(pROC)
library(mlho)
library(RJDBC)
options(java.parameters = "-Xmx8048m")

# Load all the additional packages. If this fails, just library() caret, tidyverse, and ggplot2 and add others as 
# you run into errors.
# New 6-23: Added scales. It's probably installed but you might have to install.packages("scales").
# New 6-23: Also added a couple other libraries that mlho probably already installed earlier already.
pacman::p_load(data.table, devtools, backports, Hmisc, tidyr,dplyr,ggplot2,plyr,scales,readr,
               httr, DT, lubridate, latticeExtra,devtools,LiblineaR,RJDBC, scales, tidyverse,reshape2,foreach,doParallel,caret,gbm,lubridate,praznik,install=TRUE,update=FALSE)


# Set an output directory and siteid
currSiteId <- 'MGB'
out_dir <- './'

# Load the loyalty coefficients file - set to your own path
# LOAD FROM DB
#coeff <- readr::read_csv("~/workspace/act_loyalty/LoyaltyCode_PSCoeff.csv")

# db connection for MSSQL - modify as needed
drv <- JDBC("com.microsoft.sqlserver.jdbc.SQLServerDriver",
            "/Users/jeffklann/R/sqljdbc42.jar",
            identifier.quote="`")
conn <- dbConnect(drv,
                  "jdbc:sqlserver://db.partners.org;databaseName=ACT","user","password")
# ** THIS IS FOR ORACLE
drv <- JDBC(driverClass="oracle.jdbc.OracleDriver", classPath="lib/ojdbc6.jar")
conn <- dbConnect(jdbcDriver, "jdbc:oracle:thin:@//database.hostname.com:port/service_name_or_sid", "username", "password")
# ** AN EXAMPLE OF USING ODBC instead of JDBC
#library(RODBC)
# Sys.setenv(R_MAX_NUM_DLLS = 999)
# conn <- odbcDriverConnect('driver={SQL Server};server=sv1-sqledt;database=I2B2ACT;trusted_connection=true')
# ** AN EXAMPLE OF LOADING FROM CSV FILES
# data_dir <- "~/Dropbox (Partners HealthCare)/HMS/Projects/ACT/loyalty_cohorts/mgb_returnmlho"
# dbmart <-   readr::read_csv(file.path(data_dir, "LOYALTY_MLHO_labeldt.csv"))
# dems <-   readr::read_csv(file.path(data_dir, "LOYALTY_MLHO_demographic.csv"))
# labeldt <-   readr::read_csv(file.path(data_dir, "LOYALTY_MLHO_dbmart1Y.csv"))
##end of lines to modify to get your data

# Load the the basic data needed for the study.
labeldt <- dbGetQuery(conn,paste0("select * from [dbo].[LOYALTY_MLHO_LABELDT1Y_VW]"))
dems <- dbGetQuery(conn,paste0("select * from [dbo].[LOYALTY_MLHO_DEMOGRAPHIC_VW]"))
dbmart <- dbGetQuery(conn,paste0("select * from [dbo].[LOYALTY_MLHO_DBMART_VW]"))
coeff <- dbGetQuery(conn,paste0("select * from [dbo].[XREF_LOYALTYCODE_PSCOEFF]"))

# NEW 623: Get the ACT race name from the race code in the patient dimension
# Note this only pulls patients with codes that map to ACT. We add 'other' to the others when building the demographics table.
# Assumptions: 1) race is mapped from race_cd in the patient_dimension
#              2) LOYALTY_MLHO_ARRVL has the valid list of patient_nums for the paper
races <- dbGetQuery(conn,paste0("select c_name race,patient_num from ACT_DEM_V4 dem ",
  "right join (select * from patient_dimension where patient_num in (select patient_num from dbo.LOYALTY_MLHO_ARRVL)) ", 
  "ptdim on C_DIMCODE like '%'''+ptdim.race_cd+'''%' ",
  "where c_fullname like '\\ACT\\Demographics\\Race\\%' and c_facttablecolumn='PATIENT_NUM' ")) 

# Switch column headers to lowercase
names(dems) <- tolower(names(dems))
names(dbmart) <- tolower(names(dbmart))
names(labeldt) <- tolower(names(labeldt))
names(coeff) <- tolower(names(coeff))
names(races) <- tolower(names(races))

```

Analyze the current loyalty score and its relationship to return and Charlson index

```{r stats}

#jgk - Create decils - note that decile 1 is the lowest rank!
dems <- dems %>% mutate(decile=ntile(predicted_score,10))
# Sanity check score averages by decile
dems %>% group_by(decile) %>% dplyr::summarize(aver=mean(predicted_score))


# Correlation
cor.dat <- merge(labeldt,dems,by="patient_num")
uniqpats <- c(as.character(unique(dbmart$patient_num)))
cor.dat <- cor.dat %>% mutate(label_binary=(label>0))
cor.dat <- cor.dat %>% mutate(rrate = label/length(uniqpats))

cor.test(cor.dat$predicted_score,cor.dat$label) # Loyalty score to return count
cor.scorereturn <- cor.test(cor.dat$predicted_score,as.numeric(cor.dat$label_binary)) # Score to return (binary) - for paper
cor.decilereturn <- cor.test(cor.dat$decile,as.numeric(cor.dat$label_binary)) # Decile to return (binary) - for paper
cor.scorereturn
cor.decilereturn

# Print averages of everything - not currently used
for (i in colnames(cor.dat)) {
  if (is.numeric(cor.dat[[i]])) {
    print(paste0(i,":",mean(cor.dat[[i]])))
  }
}

# Percent of patients with factor, multiply by coefficient. 
dbmart %>% group_by(phenx) %>% dplyr::summarise(ct = n())
dbmart %>% group_by(phenx) %>% dplyr::summarise(ct = n()/nrow(dems)) %>% arrange(ct)

# Compute SHAP-style importance of each variable with our data and current coefficients
# Used in importance plot at the bottom
factor.contrib <- dbmart %>% group_by(phenx) %>% dplyr::summarise(ct = n()/nrow(dems)) %>% arrange(ct) %>% inner_join(coeff,by=c("phenx"="field_name")) %>% mutate(contrib=ct*coeff) %>% arrange(contrib)
ggplot(factor.contrib %>% arrange(contrib),aes(phenx,contrib)) + geom_point() + theme(axis.text.x = element_text(angle=90))

#cor.dat %>% group_by(decile) %>% dplyr::summarise(sum(label_binary)) # count of TP (returned) by decile
#dec1 <- cor.dat %>% filter(decile==10)
#cor.test(dec1$predicted_score,as.numeric(dec1$label_binary)) # Score to return count for only top decile

# ROC curve (uses package pROC)
roc_binary <- cor.dat %>% roc("label_binary","predicted_score",ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
youden.score <- coords(roc_binary, x="best", input="threshold", best.method="youden")
#roc_binary.ci <- ci.se(roc_binary)
#plot(roc_binary.ci,type="bars")

png(file=paste0(out_dir,currSiteId, "_rocoriginal.png"))
plot.new()
plot(roc_binary,print.auc=TRUE, auc.polygon=TRUE,max.auc.polygon=TRUE,type="shape")
dev.off()

# Test code plots all score vs labels
#(p1 <- ggplot(cor.dat)+
#    geom_point(aes(x=predicted_score,y=label)))

# Charlson Index Correlations
cor.test(cor.dat$predicted_score,cor.dat$charlson_10yr_prob)
cor.test(cor.dat$predicted_score,cor.dat$charlson_index)

cor.decilecharlson <- cor.test(cor.dat$decile,cor.dat$charlson_index) # This is the one we're reporting - for paper
cor.decilecharlson

# More test code that plots way too many points
# (p <- ggplot(cor.dat)+
#   geom_point(aes(x=predicted_score,y=charlson_index)))
# (p <- ggplot(cor.dat)+
#   geom_point(aes(x=predicted_score,y=label)))
# (p <- ggplot(cor.dat)+
#   geom_point(aes(x=predicted_score,y=label_binary)))

# Mean Charlson index to mean score, grouped by index, with std. deviatin of score
# Not currently used
charlson.compare.summary <- cor.dat %>% group_by(charlson_index) %>% dplyr::summarize(mean.score = mean(predicted_score),mean.index=mean(charlson_index), sd.score = sd(predicted_score),size.score=n())

# Plot of the above                                                          #ggplot(charlson.compare.summary,aes(y=mean.score,x=mean.index,size=size.score)) +  geom_point() + geom_smooth() + geom_line(aes(x=mean.index,y=mean.score+sd.score))

# Decile of score vs charlson index, sized by group size - this goes in the paper
# Similar pattern across all deciles of loyalty score. Highest loyalty contains the sickest but not all high loyalty are sick.
charlson.compare.summary2 <- cor.dat %>% mutate(decile = ntile(predicted_score,n=10)) %>% group_by(charlson_index,decile) %>% dplyr::summarize(ct=n())
# Log scale dot size looks good, or /1000.
charlson.compare.plot <- ggplot(charlson.compare.summary2,aes(y=decile,x=charlson_index,size=log(ct))) + geom_point()
# 10-19-22 Let's save this one too
ggsave(paste0(out_dir,currSiteId,"_charlsoncompare.png"),plot=charlson.compare.plot,width=6.5,height=3,units = "in")

# Alt plot that is not as good
#ggplot(charlson.compare.summary2,aes(y=ct,x=charlson_index,color=decile)) + geom_line()

# Number of patients at each Charlson score - not currently used
#cor.dat %>% filter(predicted_score > mean(cor.dat$predicted_score)) %>% group_by(charlson_index) %>% dplyr::summarize(size.group=n())
```

MLHO modeling below:

```{r mlho}
# Set up train/test data

labeldt <- subset(labeldt,labeldt$patient_num %in% dems$patient_num)
labeldt$label <- ifelse(labeldt$label >=1,1,0)
#table(labeldt$label)

uniqpats <- c(as.character(unique(dbmart$patient_num)))
#using a 70-30 ratio
# CHANGED 3/23
test_ind <- sample(uniqpats,
                   round(.7*length(uniqpats)))

test_labels <- subset(labeldt,labeldt$patient_num %in% c(test_ind))
print("test set lables:")
table(test_labels$label)
train_labels <- subset(labeldt,!(labeldt$patient_num %in% c(test_ind)))
print("train set lables:")
table(train_labels$label)
# train and test sets
dat.train  <- subset(dbmart,!(dbmart$patient_num %in% c(test_ind)))
dat.test <- subset(dbmart,dbmart$patient_num %in% c(test_ind))

data.table::setDT(dat.train)
dat.train[,row := .I]
dat.train$value.var <- 1
uniqpats.train <- c(as.character(unique(dat.train$patient_num)))

##here is the application of MSMR.lite
dat.train <- MSMSR.lite(MLHO.dat=dat.train,
                        patients = uniqpats.train,
                        sparsity=0.005,
                        labels = labeldt,
                        topn=200, multicore=FALSE)

dat.test <- subset(dat.test,dat.test$phenx %in% colnames(dat.train))
setDT(dat.test)
dat.test[,row := .I]
dat.test$value.var <- 1
uniqpats.test <- c(as.character(unique(dat.test$patient_num)))

dat.test <- MSMSR.lite(MLHO.dat=dat.test,patients = uniqpats.test,sparsity=NA,jmi = FALSE,labels = labeldt, multicore=FALSE)

dems.save <- dems
# 12-6-22 Control for gender too
dems <- dems %>% mutate(gender.int = ifelse(gender=='F',1,0))
dems <- dplyr::select(dems,patient_num,age,gender.int)
# dems$gender < ifelse(dems$gender == "F", 1,0)

# CHANGED 3/23
model.test <- mlearn(dat.train,
                     dat.test,
                     dems=dems,
                     save.model=TRUE,
                     classifier="regLogistic", # glm
                     note="mlho_arrival_lasso",
                     cv="cv",
                     nfold=5,
                     aoi="1y_arrival",
                     multicore=FALSE)

model.test$ROC # reporting AUC for paper

# CHANGED 3/23- changed for lasso
model.coeff <- model.test$features %>% mutate(OR=exp(coefficients))
# # 9/21/22 -jgk - Run MLHO's secret code path to get regression coeffcients
# model.coeff <- mlearn(dat.train,
#                      NULL,
#                      dems=dems,
#                      save.model=FALSE,
#                      classifier="glm",
#                      note="mlho_arrival",
#                      cv="cv",
#                      nfold=5,
#                      aoi="1y_arrival",
#                      multicore=FALSE)

##from these loyalty features, we can build a model that has a AUROC of 0.81 on a held out test set
# if it is smaller than 0.85, not that great 
```

```{r Save Coefficients}
# Compute the retrained score from the coefficients to get the youden threshold
newscore <- dbmart %>% inner_join(model.test$features,by=c('phenx'='features')) %>% group_by(patient_num) %>% dplyr::summarise(score=sum(coefficients))

newscore$scaledscore=rescale(newscore$score*-1) # Scaled score -- doesn't work for prediction!
newscore$score2=newscore$score*-1

scoriness <- newscore %>% inner_join(dems.save,by='patient_num') %>% select(patient_num,score2,predicted_score,scaledscore) %>% inner_join(model.test$AE, by='patient_num')

myroc <- scoriness %>% roc("actual","score2",ci=TRUE, ci.alpha=0.9, stratified=FALSE,
 # arguments for plot
 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
 print.auc=TRUE, show.thres=TRUE)

# Compute new youden threshold
New.youden.score <- coords(myroc, x="best", input="threshold", best.method="youden")

# Compute new coeff
new.coeff <- model.test$features %>% filter(features %in% dbmart$phenx) %>% mutate(COEFF=coefficients*-1) %>% rename(FIELD_NAME=features) %>% select(FIELD_NAME,COEFF)

# Add old coeff to table
new.coeff <- new.coeff %>% inner_join(coeff,by=c('FIELD_NAME'='field_name')) %>% rename(ORIG_COEFF=coeff)

# Add Youden threshold to table
new.coeff %>% add_row(FIELD_NAME='youden',COEFF=New.youden.score$threshold, ORIG_COEFF=youden.score$threshold) 

readr::write_csv(new.coeff,file.path(out_dir,paste0(currSiteId,'_newpscoeff.csv')))
print(New.youden.score)
```

```{r Save Model}
# NEW 6-23! This is an optional step - you can save the model and demographics for safekeeping if you like - it is HUGE
results <- list(
  model.test, dems.save
)
site_results <- paste0(currSiteId, "_mlhoresults")
assign(site_results, results)
#save(list = site_results, file = file.path(out_dir, paste0(currSiteId, "_mlhoreturnresults_modelonly.rda")))
```

```{r Loyalty Dropout}
# NEW 6-23! This will create your demographic differences table and write the output as a CSV to the output directory.
# This gets the threshold from the model but you can also set it manually to match the paper:
# Change to your threshold from the paper: PITT - .321 or UKY- .532 or MGB - .613
mythreshold <- model.test$ROC$thresholdj
pts.loyal <- (model.test$AE %>% filter(Y>mythreshold))$patient_num

# Add race values loaded in the first chunk
dems.all <- dems.save %>% left_join(races,by='patient_num') %>% mutate(race = replace(race,is.na(race),"Other"))

# Make age group - thanks to this thread: https://community.rstudio.com/t/dplyr-way-s-and-base-r-way-s-of-creating-age-group-from-age/89226/2
dems.all <- dems.all %>% 
  mutate(
    # Create categories
    age_group = dplyr::case_when(
      age <= 18            ~ "0-18",
      age > 18 & age <= 34 ~ "18-34",
      age > 34 & age <= 44 ~ "35-44",
      age > 44 & age <= 54 ~ "45-54",
      age > 54 & age <= 64 ~ "55-64",
      age > 64 & age <= 84 ~ "63-84",
            age > 84             ~ "> 84"
    ),
    # Convert to factor
    age_group = factor(
      age_group,
      level = c("0-18", "18-34","35-44","45-54","55-64","63-84","> 84")
    )
  )

# Code to generate summary table
demo_summary_table <- function(dems.operate) {
  
  dems.total <- dems.operate %>% nrow() 
  
  dems.operate.gender <- dems.operate %>% group_by(gender) %>% dplyr::summarise(n = n()/dems.total) %>% mutate(Var='Gender', Cat=gender)
  dems.operate.age <- dems.operate %>% group_by(age_group) %>% dplyr::summarise(n = n()/dems.total) %>% mutate(Var="Age Group", Cat=age_group) 
    dems.operate.race <- dems.operate %>% group_by(race) %>% dplyr::summarise(n=n()/dems.total) %>% mutate(Var='Race',Cat=race)
  dems.operate.charlsoni <- dems.operate %>% dplyr::summarise(n=mean(charlson_index)) %>% mutate(Var='Charlson Index',Cat='Mean')
    dems.operate.charlson10 <- dems.operate %>% dplyr::summarise(n=mean(charlson_10yr_prob)) %>% mutate(Var='Charlson 10-year Survival Probablity',Cat='Mean')
  #%>% pivot_wider(names_from='gender',values_from='n')
  #pivot_wider(names_from='age_group',values_from='n')
    
  # Put the rows in a table
  dems.table <- bind_rows(dems.operate.gender, dems.operate.age,dems.operate.race,dems.operate.charlsoni,dems.operate.charlson10)
  
  # Add a column for percentage, round off digits, and add site id
  dems.table <- dems.table %>% left_join(dems.table %>% filter(Cat!='Mean') %>% mutate(pct=percent(n,accuracy=.1)) 
                                         %>% select(Var,Cat,pct),by=c('Var','Cat')) %>% mutate(n=round(n,digits=4)) %>% mutate(Site=currSiteId)
  
  return(dems.table %>% select(Site,Var,Cat,n,pct))
}

dems.loyal <- dems.all %>% filter(patient_num %in% pts.loyal)
dems.loyal.table <- demo_summary_table(dems.loyal)
dems.all.table <- demo_summary_table(dems.all)
dems.combined.table <- dems.loyal.table %>% left_join(dems.all.table,by=c('Var','Cat','Site')) %>% mutate(ndiff=round(n.x-n.y,digits=4),pctdiff=percent(n.x-n.y,accuracy=1)) %>% dplyr::rename(n.loyal=n.x,n.all=n.y,pct.loyal=pct.x,pct.all=pct.y)

readr::write_csv(dems.combined.table,file.path(out_dir,paste0(currSiteId,'mlhoreturn_demodiff.csv')),col_names=TRUE,append = FALSE)
```

```{r analyses}

features <- data.frame(model.test$features)

# jgk - compare MLHO features to predetermined features 
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
factor.contrib$contrib.scaled <- range01(factor.contrib$contrib)*200
features.contrib <- factor.contrib %>% inner_join(features,c("phenx"="features"))

# jgk - 9/21/22 - compute coefficients, ORs, and probabilities - something doesn't seem right with the probabilities though so we use OR
# Thank you for putting up with this messy chunk of code.
out.importance <- coeff %>% mutate(OR_original=exp(coeff),field_name=tolower(field_name)) %>% right_join(model.coeff %>% mutate(features=tolower(features)),by=c("field_name"="features")) %>% mutate(coeff_new=log(OR)) %>% mutate(prob=coeff/(1+coeff),prob_new=coeff_new/(1+coeff_new))
out.importance$OR.new.scaled <- range01(out.importance$OR)*100
out.importance$OR.original.scaled <- range01(out.importance$OR_original)*100
feature.frequency <- dbmart %>% group_by(phenx) %>% dplyr::summarise(freq = n()/nrow(dems)) %>% arrange(freq) 
out.importance <- out.importance %>% mutate(field_name=tolower(field_name)) %>% inner_join(feature.frequency %>% mutate(phenx=tolower(phenx)),by=c("field_name"="phenx"))
out.importance$ORfreq.new.scaled <- range01(out.importance$OR*out.importance$freq)*100
out.importance$ORfreq.original.scaled <- range01(out.importance$OR_original*out.importance$freq)*100

# jgk 10/6/22 - Odds ratios on sqrt scale
(imp.plot<- ggplot(out.importance) + 
    geom_segment(
      aes(y = min(sqrt(OR)),
          x = reorder(field_name,OR),
          yend = sqrt(OR),
          xend = field_name),
      size=0.5,alpha=0.5) +
    geom_point(
      aes(x=reorder(field_name,OR),y=sqrt(OR)),
      alpha=0.5,size=2,color="red") +
    geom_segment(
      aes(y = min(sqrt(OR)),
          x = reorder(field_name,OR_original),
          yend = sqrt(OR_original),
          xend = field_name),
      size=0.5,alpha=0.5) +
    geom_point(
      aes(x=reorder(field_name,OR_original),y=sqrt(OR_original)),
      alpha=0.5,size=2,color="blue") +
    geom_hline(yintercept = 1,lty=2) +
    theme_minimal()+
    coord_flip()+
    labs(y="Square root of Odds Ratio",x=""))
# CHANGED 3/2023 FILE NAME
ggsave(paste0(out_dir,currSiteId,"_importancelasso_OR.png"),plot=imp.plot,width=4,height=3,units = "in")
# 
# # jgk - 9/21/22 Scaled odds ratios
# (imp.plot<- ggplot(out.importance) + 
#     geom_segment(
#       aes(y = 0,
#           x = reorder(field_name,OR.new.scaled),
#           yend = OR.new.scaled,
#           xend = field_name),
#       size=0.5,alpha=0.5) +
#     geom_point(
#       aes(x=reorder(field_name,OR.new.scaled),y=OR.new.scaled),
#       alpha=0.5,size=2,color="red") +
#     geom_segment(
#       aes(y = 0,
#           x = reorder(field_name,OR.original.scaled),
#           yend = OR.original.scaled,
#           xend = field_name),
#       size=0.5,alpha=0.5) +
#     geom_point(
#       aes(x=reorder(field_name,OR.original.scaled),y=OR.original.scaled),
#       alpha=0.5,size=2,color="blue") +
#     theme_minimal()+
#     coord_flip()+
#     labs(y="Scaled Odds Ratio",x=""))
# ggsave(paste0(out_dir,currSiteId,"_importanceglm_scaledOR.png"),plot=imp.plot,width=4,height=3,units = "in")
# 
# # jgk - 9/21/22 tweaked for including contribution in existing score
# (imp.plot<- ggplot(out.importance) + 
#     geom_segment(
#       aes(y = 0,
#           x = reorder(field_name,ORfreq.new.scaled),
#           yend = ORfreq.new.scaled,
#           xend = field_name),
#       size=0.5,alpha=0.5) +
#     geom_point(
#       aes(x=reorder(field_name,ORfreq.new.scaled),y=ORfreq.new.scaled),
#       alpha=0.5,size=2,color="red") +
#     geom_segment(
#       aes(y = 0,
#           x = reorder(field_name,ORfreq.original.scaled),
#           yend = ORfreq.original.scaled,
#           xend = field_name),
#       size=0.5,alpha=0.5) +
#     geom_point(
#       aes(x=reorder(field_name,ORfreq.original.scaled),y=ORfreq.original.scaled),
#       alpha=0.5,size=2,color="blue") +
#     theme_minimal()+
#     coord_flip()+
#     labs(y="Scaled Frequency* Odds Ratio)",x=""))
# ggsave(paste0(out_dir,currSiteId,"_importanceglm_freqscaledOR.png"),plot=imp.plot,width=4,height=3,units = "in")


# ROC curve (uses package pROC) and counts
library(pROC)
roc_mlho <- model.test$AE %>% roc("actual","Y",ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
youden.score_mlho <- coords(roc_binary, x="best", input="threshold", best.method="youden")
youden.score
# CHANGED 3/23 - This was an error, the actual scores change after retraining, so you have to recompute. Here we use the test set only.
cnt.all <- dems %>% nrow()
cnt.youden <- cor.dat %>% filter(predicted_score>youden.score$threshold) %>% select(patient_num) %>% distinct_all() %>% nrow()
cnt.true <- cor.dat %>% filter(label_binary==TRUE) %>% select(patient_num) %>% distinct_all() %>% nrow()
cnt.youdenmlho <- model.test$AE %>% filter(Y>model.test$ROC$thresholdj) %>% nrow() 
cnt.testset <- model.test$AE %>% nrow()
coeff.mlho.max <- sum((model.test$features %>% select(coefficients) %>% filter(coefficients>=0))$coefficients)
coeff.mlho.min <- sum((model.test$features %>% select(coefficients) %>% filter(coefficients<=0))$coefficients)
# Percent of patients with factor
prevalences <- dbmart %>% group_by(phenx) %>% dplyr::summarise(ct = n()/nrow(dems)) %>% arrange(ct) 
#cnt.youdenmlho <- cor.dat %>% filter(predicted_score>youden.score_mlho$threshold) %>% select(patient_num) %>% distinct_all() %>% nrow()
#roc_binary.ci <- ci.se(roc_binary)
 
# Alternative using plotROC...
# d is observed, m is predicted
#install.packages("plotROC")
# library(plotROC)
# roc.data <- model.test$AE
#     (g <- ggplot(roc.data, aes(m=Y, d=actual)) + #factor(actual, levels = c(1,0)))) +
#         geom_roc(n.cuts=.1) + 
#         coord_equal() +
#         style_roc() ) 
#         #annotate("text", x=0.75, y=0.25, label=paste("AUC =", round(ROC$roc, 4))))#round((calc_auc(g))$AUC, 4)
#     ggsave(filename=paste(dir,"plotROC.png",sep=''))

png(file=paste0(out_dir,currSiteId, "_rocmlho.png"))
plot.new()
plot(roc_mlho, print.auc=TRUE, auc.polygon=TRUE,max.auc.polygon=TRUE,type="shape")
dev.off()

# Save the ROC curves overlaid on each other
png(file=paste0(out_dir,currSiteId, "_rocall.png"))
plot.new()
plot(roc_binary, print.auc = FALSE, col = "blue",auc.polygon=FALSE, type="shape")
plot(roc_mlho, print.auc = FALSE, col = "red", print.auc.y = .4, auc.polygon=FALSE, type="shape",add = TRUE) 
text(0.25, 0.4, paste("AUC for original:", round(roc_binary$auc, 3)),col="blue")
text(0.25, 0.3, paste("AUC for MLHO:", round(roc_mlho$auc, 3)),col="red")
dev.off()
png(file=paste0(out_dir,currSiteId, "_rocall_smooth.png"))
plot.new()
plot(roc_binary, print.auc = FALSE, col = "blue",auc.polygon=FALSE)
plot(roc_mlho, print.auc = FALSE, col = "red", print.auc.y = .4, auc.polygon=FALSE,add = TRUE) 
text(0.25, 0.4, paste("AUC for original:", round(roc_binary$auc, 3)),col="blue")
text(0.25, 0.3, paste("AUC for MLHO:", round(roc_mlho$auc, 3)),col="red")
dev.off()

```
```{r Save}
results <- list(
  #model.test = model.test, # MLHO model - this is a ton of data
  model.roc = model.test$ROC, # MLHO ROC stats
  original.auc = as.numeric(roc_binary$auc), # Score AUC
  youden.score = youden.score, # Youden point for ROC of score
  youden.score_mlho = youden.score_mlho, # Youden point of MLHO ROC
  features = features, # Importance
  importance = out.importance, # 9-21-22 Importance
  charlson.compare.summary2 = charlson.compare.summary2, # Charlson comparison plot data
  features.contrib = features.contrib,  # Importance in original vs. MLHO
  cor.decilecharlson = cor.decilecharlson, # Correlations...
  cor.scorereturn = cor.scorereturn,
  cor.decilereturn = cor.decilereturn,
  # Added 12-6-22
  pct.truesize = cnt.true/cnt.all,
  pct.youdensize = cnt.youden/cnt.all,
  #pct.youdenmlhosize = cnt.youdenmlho / cnt.all,
  cnt.all = round(cnt.all,-2),
  # Added 3/2023
  pct.youdenmlhosize = cnt.youdenmlho / cnt.testset,
  coeff.mlho.min = coeff.mlho.min,
  coeff.mlho.max = coeff.mlho.max,
  prevalences = prevalences
)
site_results <- paste0(currSiteId, "_mlhoresults")
assign(site_results, results)
save(list = site_results, file = file.path(out_dir, paste0(currSiteId, "_mlhoreturnresults.rda")))
```

